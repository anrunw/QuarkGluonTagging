{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(sys.path[0]),'lib'))\n",
    "\n",
    "from train import parse_config, get_features, print_model_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, Nadam\n",
    "from callbacks import all_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from ../train/train_config_gru.yml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Inputs': ['j1_ptrel',\n",
       "  'j1_etarot',\n",
       "  'j1_phirot',\n",
       "  'j1_erel',\n",
       "  'j1_deltaR',\n",
       "  'j1_pdgid',\n",
       "  'j_index'],\n",
       " 'Labels': ['j_g', 'j_q', 'j_w', 'j_z', 'j_t', 'j_index'],\n",
       " 'KerasModel': 'gru_model',\n",
       " 'KerasModelRetrain': 'gru_model_constraint',\n",
       " 'KerasLoss': 'categorical_crossentropy',\n",
       " 'L1Reg': 0.0001,\n",
       " 'L1RegR': 0.001,\n",
       " 'NormalizeInputs': 1,\n",
       " 'InputType': 'Conv1D',\n",
       " 'MaxParticles': 20}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from collections import namedtuple\n",
    "Option = namedtuple(\"MyStruct\", \"inputModel inputFile tree config jsonModel\")\n",
    "\n",
    "options = Option(\n",
    "    inputModel = '../KERAS_gru_model_weights.h5',\n",
    "    inputFile = '../data/processed-pythia82-lhc13-all-pt1-50k-r1_h022_e0175_t220_nonu_withPars_truth_0.z',\n",
    "    tree = 't_allpar_new',\n",
    "    config = '../train/train_config_gru.yml',\n",
    "    jsonModel = '../KERAS_gru_model.json'\n",
    ")\n",
    "\n",
    "print(\"Loading configuration from\", options.config)\n",
    "config = open(options.config, 'r')\n",
    "yamlConfig =  yaml.load(config, Loader=yaml.FullLoader)\n",
    "\n",
    "yamlConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5131613,)\n",
      "('index', 'j_ptfrac', 'j_pt', 'j_eta', 'j_mass', 'j_tau1_b1', 'j_tau2_b1', 'j_tau3_b1', 'j_tau1_b2', 'j_tau2_b2', 'j_tau3_b2', 'j_tau32_b1', 'j_tau32_b2', 'j_zlogz', 'j_c1_b0', 'j_c1_b1', 'j_c1_b2', 'j_c2_b1', 'j_c2_b2', 'j_d2_b1', 'j_d2_b2', 'j_d2_a1_b1', 'j_d2_a1_b2', 'j_m2_b1', 'j_m2_b2', 'j_n2_b1', 'j_n2_b2', 'j_tau1_b1_mmdt', 'j_tau2_b1_mmdt', 'j_tau3_b1_mmdt', 'j_tau1_b2_mmdt', 'j_tau2_b2_mmdt', 'j_tau3_b2_mmdt', 'j_tau32_b1_mmdt', 'j_tau32_b2_mmdt', 'j_c1_b0_mmdt', 'j_c1_b1_mmdt', 'j_c1_b2_mmdt', 'j_c2_b1_mmdt', 'j_c2_b2_mmdt', 'j_d2_b1_mmdt', 'j_d2_b2_mmdt', 'j_d2_a1_b1_mmdt', 'j_d2_a1_b2_mmdt', 'j_m2_b1_mmdt', 'j_m2_b2_mmdt', 'j_n2_b1_mmdt', 'j_n2_b2_mmdt', 'j_mass_trim', 'j_mass_mmdt', 'j_mass_prun', 'j_mass_sdb2', 'j_mass_sdm1', 'j_multiplicity', 'j1_px', 'j1_py', 'j1_pz', 'j1_e', 'j1_pdgid', 'j1_erel', 'j1_pt', 'j1_ptrel', 'j1_eta', 'j1_etarel', 'j1_etarot', 'j1_phi', 'j1_phirel', 'j1_phirot', 'j1_deltaR', 'j1_costheta', 'j1_costhetarel', 'j1_e1mcosthetarel', 'j_g', 'j_q', 'j_w', 'j_z', 'j_t', 'j_undef', 'j_index')\n",
      "(98769, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train_val, X_test, y_train_val, y_test, labels  = get_features(options, yamlConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(options.jsonModel, 'r')\n",
    "model = model_from_json(json_file.read())\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20, 6)             0         \n",
      "_________________________________________________________________\n",
      "gru_selu (GRU)               (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "dense_relu (Dense)           (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "rnn_densef (Dense)           (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 2,145\n",
      "Trainable params: 2,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20, 6)             0         \n",
      "_________________________________________________________________\n",
      "gru_selu (GRU)               (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "dense_relu (Dense)           (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "rnn_densef (Dense)           (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 2,145\n",
      "Trainable params: 2,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 59261 samples, validate on 19754 samples\n",
      "Epoch 1/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 3.0059 - acc: 0.2858\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00000: val_loss improved from inf to 1.92869, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00000: val_loss improved from inf to 1.92869, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00000: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00000: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 2s - loss: 2.9575 - acc: 0.2877 - val_loss: 1.9287 - val_acc: 0.3515\n",
      "Epoch 2/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.7901 - acc: 0.3723\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00001: val_loss improved from 1.92869 to 1.62100, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00001: val_loss improved from 1.92869 to 1.62100, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00001: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00001: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.7821 - acc: 0.3735 - val_loss: 1.6210 - val_acc: 0.3999\n",
      "Epoch 3/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.5973 - acc: 0.4072\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00002: val_loss improved from 1.62100 to 1.51435, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00002: val_loss improved from 1.62100 to 1.51435, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00002: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00002: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.5933 - acc: 0.4089 - val_loss: 1.5144 - val_acc: 0.4251\n",
      "Epoch 4/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.5158 - acc: 0.4263\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00003: val_loss improved from 1.51435 to 1.46174, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00003: val_loss improved from 1.51435 to 1.46174, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00003: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00003: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.5147 - acc: 0.4267 - val_loss: 1.4617 - val_acc: 0.4382\n",
      "Epoch 5/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.4691 - acc: 0.4381\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00004: val_loss improved from 1.46174 to 1.42687, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00004: val_loss improved from 1.46174 to 1.42687, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00004: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00004: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.4679 - acc: 0.4383 - val_loss: 1.4269 - val_acc: 0.4497\n",
      "Epoch 6/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.4397 - acc: 0.4482\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00005: val_loss improved from 1.42687 to 1.39892, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00005: val_loss improved from 1.42687 to 1.39892, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00005: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00005: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.4382 - acc: 0.4482 - val_loss: 1.3989 - val_acc: 0.4567\n",
      "Epoch 7/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.4133 - acc: 0.4552\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00006: val_loss improved from 1.39892 to 1.37576, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00006: val_loss improved from 1.39892 to 1.37576, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00006: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00006: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.4115 - acc: 0.4557 - val_loss: 1.3758 - val_acc: 0.4648\n",
      "Epoch 8/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.3896 - acc: 0.4630\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00007: val_loss improved from 1.37576 to 1.35614, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00007: val_loss improved from 1.37576 to 1.35614, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00007: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00007: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3888 - acc: 0.4633 - val_loss: 1.3561 - val_acc: 0.4741\n",
      "Epoch 9/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.3681 - acc: 0.4703\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00008: val_loss improved from 1.35614 to 1.33918, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00008: val_loss improved from 1.35614 to 1.33918, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00008: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00008: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3676 - acc: 0.4706 - val_loss: 1.3392 - val_acc: 0.4831\n",
      "Epoch 10/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.3528 - acc: 0.4783\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00009: val_loss improved from 1.33918 to 1.32431, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00009: val_loss improved from 1.33918 to 1.32431, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00009: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00009: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00009: saving model to .\\training_callbacks/KERAS_check_model_epoch09.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3512 - acc: 0.4788 - val_loss: 1.3243 - val_acc: 0.4885\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.3381 - acc: 0.4827\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00010: val_loss improved from 1.32431 to 1.31119, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00010: val_loss improved from 1.32431 to 1.31119, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00010: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00010: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3382 - acc: 0.4822 - val_loss: 1.3112 - val_acc: 0.4942\n",
      "Epoch 12/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.3251 - acc: 0.4866\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00011: val_loss improved from 1.31119 to 1.29946, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00011: val_loss improved from 1.31119 to 1.29946, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00011: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00011: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3238 - acc: 0.4873 - val_loss: 1.2995 - val_acc: 0.5016\n",
      "Epoch 13/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.3121 - acc: 0.4932\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00012: val_loss improved from 1.29946 to 1.28876, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00012: val_loss improved from 1.29946 to 1.28876, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00012: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00012: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3120 - acc: 0.4929 - val_loss: 1.2888 - val_acc: 0.5083\n",
      "Epoch 14/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.3013 - acc: 0.4976\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00013: val_loss improved from 1.28876 to 1.27893, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00013: val_loss improved from 1.28876 to 1.27893, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00013: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00013: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3014 - acc: 0.4974 - val_loss: 1.2789 - val_acc: 0.5125\n",
      "Epoch 15/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2918 - acc: 0.5010\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00014: val_loss improved from 1.27893 to 1.26996, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00014: val_loss improved from 1.27893 to 1.26996, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00014: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00014: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2921 - acc: 0.5014 - val_loss: 1.2700 - val_acc: 0.5176\n",
      "Epoch 16/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2846 - acc: 0.5085\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00015: val_loss improved from 1.26996 to 1.26163, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00015: val_loss improved from 1.26996 to 1.26163, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00015: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00015: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2850 - acc: 0.5083 - val_loss: 1.2616 - val_acc: 0.5216\n",
      "Epoch 17/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2756 - acc: 0.5104\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00016: val_loss improved from 1.26163 to 1.25369, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00016: val_loss improved from 1.26163 to 1.25369, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00016: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00016: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2762 - acc: 0.5104 - val_loss: 1.2537 - val_acc: 0.5253\n",
      "Epoch 18/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2695 - acc: 0.5132\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00017: val_loss improved from 1.25369 to 1.24632, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00017: val_loss improved from 1.25369 to 1.24632, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00017: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00017: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2685 - acc: 0.5138 - val_loss: 1.2463 - val_acc: 0.5286\n",
      "Epoch 19/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.2612 - acc: 0.5181\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00018: val_loss improved from 1.24632 to 1.23945, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00018: val_loss improved from 1.24632 to 1.23945, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00018: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00018: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2615 - acc: 0.5179 - val_loss: 1.2395 - val_acc: 0.5333\n",
      "Epoch 20/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2549 - acc: 0.5209\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00019: val_loss improved from 1.23945 to 1.23307, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00019: val_loss improved from 1.23945 to 1.23307, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00019: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00019: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00019: saving model to .\\training_callbacks/KERAS_check_model_epoch19.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2543 - acc: 0.5213 - val_loss: 1.2331 - val_acc: 0.5356\n",
      "Epoch 21/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.2501 - acc: 0.5234\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00020: val_loss improved from 1.23307 to 1.22699, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00020: val_loss improved from 1.23307 to 1.22699, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00020: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00020: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2492 - acc: 0.5239 - val_loss: 1.2270 - val_acc: 0.5381\n",
      "Epoch 22/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2435 - acc: 0.5259\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00021: val_loss improved from 1.22699 to 1.22158, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00021: val_loss improved from 1.22699 to 1.22158, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00021: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00021: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2432 - acc: 0.5262 - val_loss: 1.2216 - val_acc: 0.5412\n",
      "Epoch 23/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2388 - acc: 0.5289\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00022: val_loss improved from 1.22158 to 1.21585, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00022: val_loss improved from 1.22158 to 1.21585, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00022: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00022: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2364 - acc: 0.5297 - val_loss: 1.2159 - val_acc: 0.5441\n",
      "Epoch 24/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2324 - acc: 0.5309\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00023: val_loss improved from 1.21585 to 1.21072, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00023: val_loss improved from 1.21585 to 1.21072, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00023: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00023: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2320 - acc: 0.5311 - val_loss: 1.2107 - val_acc: 0.5457\n",
      "Epoch 25/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2281 - acc: 0.5346\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00024: val_loss improved from 1.21072 to 1.20561, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00024: val_loss improved from 1.21072 to 1.20561, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00024: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00024: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2275 - acc: 0.5346 - val_loss: 1.2056 - val_acc: 0.5485\n",
      "Epoch 26/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2222 - acc: 0.5374- ETA: 0s - loss: 1.2251 - acc: \n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00025: val_loss improved from 1.20561 to 1.20103, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00025: val_loss improved from 1.20561 to 1.20103, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00025: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00025: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2214 - acc: 0.5370 - val_loss: 1.2010 - val_acc: 0.5508\n",
      "Epoch 27/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2190 - acc: 0.5390\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00026: val_loss improved from 1.20103 to 1.19646, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00026: val_loss improved from 1.20103 to 1.19646, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00026: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00026: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2188 - acc: 0.5393 - val_loss: 1.1965 - val_acc: 0.5532\n",
      "Epoch 28/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2148 - acc: 0.5393\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00027: val_loss improved from 1.19646 to 1.19223, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00027: val_loss improved from 1.19646 to 1.19223, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00027: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00027: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2134 - acc: 0.5397 - val_loss: 1.1922 - val_acc: 0.5550\n",
      "Epoch 29/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2083 - acc: 0.5409\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00028: val_loss improved from 1.19223 to 1.18834, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00028: val_loss improved from 1.19223 to 1.18834, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00028: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00028: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2081 - acc: 0.5415 - val_loss: 1.1883 - val_acc: 0.5568\n",
      "Epoch 30/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2065 - acc: 0.5442\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00029: val_loss improved from 1.18834 to 1.18369, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00029: val_loss improved from 1.18834 to 1.18369, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00029: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00029: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00029: saving model to .\\training_callbacks/KERAS_check_model_epoch29.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2056 - acc: 0.5448 - val_loss: 1.1837 - val_acc: 0.5610\n",
      "Epoch 31/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2013 - acc: 0.5470\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00030: val_loss improved from 1.18369 to 1.17972, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00030: val_loss improved from 1.18369 to 1.17972, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00030: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00030: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2016 - acc: 0.5469 - val_loss: 1.1797 - val_acc: 0.5631\n",
      "Epoch 32/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1984 - acc: 0.5469\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00031: val_loss improved from 1.17972 to 1.17605, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00031: val_loss improved from 1.17972 to 1.17605, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00031: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00031: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1986 - acc: 0.5467 - val_loss: 1.1761 - val_acc: 0.5633\n",
      "Epoch 33/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1940 - acc: 0.5507\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00032: val_loss improved from 1.17605 to 1.17208, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00032: val_loss improved from 1.17605 to 1.17208, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00032: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00032: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1936 - acc: 0.5508 - val_loss: 1.1721 - val_acc: 0.5662\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1894 - acc: 0.5536\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00033: val_loss improved from 1.17208 to 1.16813, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00033: val_loss improved from 1.17208 to 1.16813, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00033: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00033: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1901 - acc: 0.5537 - val_loss: 1.1681 - val_acc: 0.5674\n",
      "Epoch 35/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1862 - acc: 0.5542\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00034: val_loss improved from 1.16813 to 1.16460, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00034: val_loss improved from 1.16813 to 1.16460, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00034: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00034: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1860 - acc: 0.5534 - val_loss: 1.1646 - val_acc: 0.5687\n",
      "Epoch 36/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1822 - acc: 0.5552\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00035: val_loss improved from 1.16460 to 1.16105, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00035: val_loss improved from 1.16460 to 1.16105, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00035: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00035: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1823 - acc: 0.5552 - val_loss: 1.1610 - val_acc: 0.5700\n",
      "Epoch 37/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1794 - acc: 0.5563\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00036: val_loss improved from 1.16105 to 1.15755, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00036: val_loss improved from 1.16105 to 1.15755, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00036: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00036: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1793 - acc: 0.5568 - val_loss: 1.1575 - val_acc: 0.5715\n",
      "Epoch 38/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.1767 - acc: 0.5581\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00037: val_loss improved from 1.15755 to 1.15413, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00037: val_loss improved from 1.15755 to 1.15413, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00037: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00037: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1768 - acc: 0.5582 - val_loss: 1.1541 - val_acc: 0.5727\n",
      "Epoch 39/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1728 - acc: 0.5581\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00038: val_loss improved from 1.15413 to 1.15086, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00038: val_loss improved from 1.15413 to 1.15086, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00038: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00038: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1729 - acc: 0.5586 - val_loss: 1.1509 - val_acc: 0.5741\n",
      "Epoch 40/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1698 - acc: 0.5622\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00039: val_loss improved from 1.15086 to 1.14754, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00039: val_loss improved from 1.15086 to 1.14754, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00039: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00039: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00039: saving model to .\\training_callbacks/KERAS_check_model_epoch39.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1693 - acc: 0.5628 - val_loss: 1.1475 - val_acc: 0.5763\n",
      "Epoch 41/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1666 - acc: 0.5639\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00040: val_loss improved from 1.14754 to 1.14418, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00040: val_loss improved from 1.14754 to 1.14418, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00040: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00040: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1653 - acc: 0.5642 - val_loss: 1.1442 - val_acc: 0.5786\n",
      "Epoch 42/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1641 - acc: 0.5660\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00041: val_loss improved from 1.14418 to 1.14097, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00041: val_loss improved from 1.14418 to 1.14097, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00041: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00041: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1634 - acc: 0.5662 - val_loss: 1.1410 - val_acc: 0.5775\n",
      "Epoch 43/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1599 - acc: 0.5662\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00042: val_loss improved from 1.14097 to 1.13772, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00042: val_loss improved from 1.14097 to 1.13772, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00042: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00042: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1597 - acc: 0.5660 - val_loss: 1.1377 - val_acc: 0.5797\n",
      "Epoch 44/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.1579 - acc: 0.5683\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00043: val_loss improved from 1.13772 to 1.13502, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00043: val_loss improved from 1.13772 to 1.13502, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00043: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00043: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1571 - acc: 0.5688 - val_loss: 1.1350 - val_acc: 0.5807\n",
      "Epoch 45/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1555 - acc: 0.5702- ETA: 0s - loss: 1.1578 \n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00044: val_loss improved from 1.13502 to 1.13192, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00044: val_loss improved from 1.13502 to 1.13192, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00044: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00044: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1551 - acc: 0.5698 - val_loss: 1.1319 - val_acc: 0.5816\n",
      "Epoch 46/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1532 - acc: 0.5704\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00045: val_loss improved from 1.13192 to 1.12890, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00045: val_loss improved from 1.13192 to 1.12890, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00045: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00045: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1524 - acc: 0.5703 - val_loss: 1.1289 - val_acc: 0.5833\n",
      "Epoch 47/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1475 - acc: 0.5727\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00046: val_loss improved from 1.12890 to 1.12622, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00046: val_loss improved from 1.12890 to 1.12622, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00046: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00046: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1477 - acc: 0.5728 - val_loss: 1.1262 - val_acc: 0.5849\n",
      "Epoch 48/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1459 - acc: 0.5731\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00047: val_loss improved from 1.12622 to 1.12338, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00047: val_loss improved from 1.12622 to 1.12338, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00047: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00047: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1460 - acc: 0.5728 - val_loss: 1.1234 - val_acc: 0.5867\n",
      "Epoch 49/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1437 - acc: 0.5747\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00048: val_loss improved from 1.12338 to 1.12094, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00048: val_loss improved from 1.12338 to 1.12094, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00048: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00048: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1438 - acc: 0.5747 - val_loss: 1.1209 - val_acc: 0.5878\n",
      "Epoch 50/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1397 - acc: 0.5764\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00049: val_loss improved from 1.12094 to 1.11821, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00049: val_loss improved from 1.12094 to 1.11821, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00049: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00049: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00049: saving model to .\\training_callbacks/KERAS_check_model_epoch49.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1401 - acc: 0.5766 - val_loss: 1.1182 - val_acc: 0.5908\n",
      "Epoch 51/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1370 - acc: 0.5766\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00050: val_loss improved from 1.11821 to 1.11568, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00050: val_loss improved from 1.11821 to 1.11568, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00050: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00050: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1373 - acc: 0.5763 - val_loss: 1.1157 - val_acc: 0.5910\n",
      "Epoch 52/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1357 - acc: 0.5774\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00051: val_loss improved from 1.11568 to 1.11330, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00051: val_loss improved from 1.11568 to 1.11330, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00051: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00051: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1363 - acc: 0.5776 - val_loss: 1.1133 - val_acc: 0.5923\n",
      "Epoch 53/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1340 - acc: 0.5820\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00052: val_loss improved from 1.11330 to 1.11115, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00052: val_loss improved from 1.11330 to 1.11115, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00052: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00052: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1340 - acc: 0.5816 - val_loss: 1.1112 - val_acc: 0.5929\n",
      "Epoch 54/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1297 - acc: 0.5818\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00053: val_loss improved from 1.11115 to 1.10893, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00053: val_loss improved from 1.11115 to 1.10893, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00053: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00053: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1303 - acc: 0.5818 - val_loss: 1.1089 - val_acc: 0.5937\n",
      "Epoch 55/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1290 - acc: 0.5843\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00054: val_loss improved from 1.10893 to 1.10698, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00054: val_loss improved from 1.10893 to 1.10698, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00054: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00054: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1294 - acc: 0.5839 - val_loss: 1.1070 - val_acc: 0.5947\n",
      "Epoch 56/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1262 - acc: 0.5840\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00055: val_loss improved from 1.10698 to 1.10441, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00055: val_loss improved from 1.10698 to 1.10441, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00055: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00055: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1268 - acc: 0.5841 - val_loss: 1.1044 - val_acc: 0.5959\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1249 - acc: 0.5856\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00056: val_loss improved from 1.10441 to 1.10260, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00056: val_loss improved from 1.10441 to 1.10260, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00056: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00056: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1252 - acc: 0.5856 - val_loss: 1.1026 - val_acc: 0.5969\n",
      "Epoch 58/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1242 - acc: 0.5854\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00057: val_loss improved from 1.10260 to 1.10032, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00057: val_loss improved from 1.10260 to 1.10032, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00057: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00057: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1243 - acc: 0.5858 - val_loss: 1.1003 - val_acc: 0.5978\n",
      "Epoch 59/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1199 - acc: 0.5880\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00058: val_loss improved from 1.10032 to 1.09867, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00058: val_loss improved from 1.10032 to 1.09867, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00058: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00058: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1201 - acc: 0.5878 - val_loss: 1.0987 - val_acc: 0.5987\n",
      "Epoch 60/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1198 - acc: 0.5900- ETA: 0s - loss: 1.1200 - acc: 0.589\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00059: val_loss improved from 1.09867 to 1.09622, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00059: val_loss improved from 1.09867 to 1.09622, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00059: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00059: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00059: saving model to .\\training_callbacks/KERAS_check_model_epoch59.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1195 - acc: 0.5895 - val_loss: 1.0962 - val_acc: 0.6010\n",
      "Epoch 61/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1151 - acc: 0.5905\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00060: val_loss improved from 1.09622 to 1.09435, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00060: val_loss improved from 1.09622 to 1.09435, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00060: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00060: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1167 - acc: 0.5898 - val_loss: 1.0943 - val_acc: 0.6021\n",
      "Epoch 62/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1159 - acc: 0.5906\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00061: val_loss improved from 1.09435 to 1.09231, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00061: val_loss improved from 1.09435 to 1.09231, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00061: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00061: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1151 - acc: 0.5910 - val_loss: 1.0923 - val_acc: 0.6029\n",
      "Epoch 63/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1142 - acc: 0.5909\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00062: val_loss improved from 1.09231 to 1.09037, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00062: val_loss improved from 1.09231 to 1.09037, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00062: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00062: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1141 - acc: 0.5908 - val_loss: 1.0904 - val_acc: 0.6039\n",
      "Epoch 64/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1103 - acc: 0.5940\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00063: val_loss improved from 1.09037 to 1.08867, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00063: val_loss improved from 1.09037 to 1.08867, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00063: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00063: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1112 - acc: 0.5935 - val_loss: 1.0887 - val_acc: 0.6046\n",
      "Epoch 65/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1093 - acc: 0.5923\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00064: val_loss improved from 1.08867 to 1.08720, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00064: val_loss improved from 1.08867 to 1.08720, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00064: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00064: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1096 - acc: 0.5926 - val_loss: 1.0872 - val_acc: 0.6054\n",
      "Epoch 66/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1083 - acc: 0.5952\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00065: val_loss improved from 1.08720 to 1.08476, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00065: val_loss improved from 1.08720 to 1.08476, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00065: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00065: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1076 - acc: 0.5954 - val_loss: 1.0848 - val_acc: 0.6071\n",
      "Epoch 67/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1067 - acc: 0.5955\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00066: val_loss improved from 1.08476 to 1.08323, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00066: val_loss improved from 1.08476 to 1.08323, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00066: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00066: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1055 - acc: 0.5958 - val_loss: 1.0832 - val_acc: 0.6080\n",
      "Epoch 68/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1037 - acc: 0.5984\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00067: val_loss improved from 1.08323 to 1.08116, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00067: val_loss improved from 1.08323 to 1.08116, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00067: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00067: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1043 - acc: 0.5986 - val_loss: 1.0812 - val_acc: 0.6085\n",
      "Epoch 69/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1020 - acc: 0.5973\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00068: val_loss improved from 1.08116 to 1.07945, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00068: val_loss improved from 1.08116 to 1.07945, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00068: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00068: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1013 - acc: 0.5981 - val_loss: 1.0794 - val_acc: 0.6093\n",
      "Epoch 70/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1010 - acc: 0.5987\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00069: val_loss improved from 1.07945 to 1.07769, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00069: val_loss improved from 1.07945 to 1.07769, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00069: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00069: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00069: saving model to .\\training_callbacks/KERAS_check_model_epoch69.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1013 - acc: 0.5989 - val_loss: 1.0777 - val_acc: 0.6102\n",
      "Epoch 71/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0990 - acc: 0.6015\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00070: val_loss improved from 1.07769 to 1.07598, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00070: val_loss improved from 1.07769 to 1.07598, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00070: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00070: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0995 - acc: 0.6019 - val_loss: 1.0760 - val_acc: 0.6123\n",
      "Epoch 72/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0962 - acc: 0.6030\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00071: val_loss improved from 1.07598 to 1.07442, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00071: val_loss improved from 1.07598 to 1.07442, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00071: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00071: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0972 - acc: 0.6025 - val_loss: 1.0744 - val_acc: 0.6140\n",
      "Epoch 73/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0947 - acc: 0.6043\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00072: val_loss improved from 1.07442 to 1.07273, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00072: val_loss improved from 1.07442 to 1.07273, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00072: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00072: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0954 - acc: 0.6036 - val_loss: 1.0727 - val_acc: 0.6149\n",
      "Epoch 74/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0946 - acc: 0.6032\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00073: val_loss improved from 1.07273 to 1.07135, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00073: val_loss improved from 1.07273 to 1.07135, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00073: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00073: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0939 - acc: 0.6035 - val_loss: 1.0714 - val_acc: 0.6155\n",
      "Epoch 75/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.0923 - acc: 0.6050\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00074: val_loss improved from 1.07135 to 1.06960, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00074: val_loss improved from 1.07135 to 1.06960, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00074: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00074: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0918 - acc: 0.6051 - val_loss: 1.0696 - val_acc: 0.6159\n",
      "Epoch 76/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0909 - acc: 0.6048\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00075: val_loss improved from 1.06960 to 1.06818, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00075: val_loss improved from 1.06960 to 1.06818, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00075: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00075: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0914 - acc: 0.6040 - val_loss: 1.0682 - val_acc: 0.6183\n",
      "Epoch 77/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0896 - acc: 0.6069\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00076: val_loss improved from 1.06818 to 1.06647, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00076: val_loss improved from 1.06818 to 1.06647, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00076: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00076: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0890 - acc: 0.6068 - val_loss: 1.0665 - val_acc: 0.6189\n",
      "Epoch 78/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0859 - acc: 0.6085\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00077: val_loss improved from 1.06647 to 1.06458, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00077: val_loss improved from 1.06647 to 1.06458, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00077: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00077: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0870 - acc: 0.6088 - val_loss: 1.0646 - val_acc: 0.6201\n",
      "Epoch 79/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0850 - acc: 0.6101\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00078: val_loss improved from 1.06458 to 1.06290, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00078: val_loss improved from 1.06458 to 1.06290, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00078: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00078: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0862 - acc: 0.6096 - val_loss: 1.0629 - val_acc: 0.6217\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0859 - acc: 0.6081\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00079: val_loss improved from 1.06290 to 1.06118, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00079: val_loss improved from 1.06290 to 1.06118, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00079: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00079: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00079: saving model to .\\training_callbacks/KERAS_check_model_epoch79.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0850 - acc: 0.6077 - val_loss: 1.0612 - val_acc: 0.6225\n",
      "Epoch 81/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0842 - acc: 0.6111\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00080: val_loss improved from 1.06118 to 1.05975, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00080: val_loss improved from 1.06118 to 1.05975, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00080: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00080: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0837 - acc: 0.6117 - val_loss: 1.0598 - val_acc: 0.6232\n",
      "Epoch 82/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0813 - acc: 0.6110\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00081: val_loss improved from 1.05975 to 1.05820, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00081: val_loss improved from 1.05975 to 1.05820, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00081: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00081: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0820 - acc: 0.6116 - val_loss: 1.0582 - val_acc: 0.6249\n",
      "Epoch 83/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0811 - acc: 0.6128\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00082: val_loss improved from 1.05820 to 1.05647, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00082: val_loss improved from 1.05820 to 1.05647, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00082: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00082: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0804 - acc: 0.6135 - val_loss: 1.0565 - val_acc: 0.6250\n",
      "Epoch 84/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0766 - acc: 0.6141\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00083: val_loss improved from 1.05647 to 1.05477, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00083: val_loss improved from 1.05647 to 1.05477, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00083: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00083: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0781 - acc: 0.6138 - val_loss: 1.0548 - val_acc: 0.6248\n",
      "Epoch 85/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0753 - acc: 0.6161\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00084: val_loss improved from 1.05477 to 1.05323, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00084: val_loss improved from 1.05477 to 1.05323, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00084: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00084: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0766 - acc: 0.6154 - val_loss: 1.0532 - val_acc: 0.6270\n",
      "Epoch 86/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0735 - acc: 0.6160\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00085: val_loss improved from 1.05323 to 1.05164, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00085: val_loss improved from 1.05323 to 1.05164, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00085: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00085: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0740 - acc: 0.6157 - val_loss: 1.0516 - val_acc: 0.6284\n",
      "Epoch 87/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0742 - acc: 0.6176\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00086: val_loss improved from 1.05164 to 1.05027, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00086: val_loss improved from 1.05164 to 1.05027, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00086: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00086: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0737 - acc: 0.6176 - val_loss: 1.0503 - val_acc: 0.6306\n",
      "Epoch 88/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0729 - acc: 0.6188\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00087: val_loss improved from 1.05027 to 1.04861, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00087: val_loss improved from 1.05027 to 1.04861, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00087: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00087: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0733 - acc: 0.6183 - val_loss: 1.0486 - val_acc: 0.6316\n",
      "Epoch 89/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0706 - acc: 0.6197- ETA: 0s - loss: 1.0702 - acc: 0.61\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00088: val_loss improved from 1.04861 to 1.04675, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00088: val_loss improved from 1.04861 to 1.04675, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00088: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00088: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0703 - acc: 0.6194 - val_loss: 1.0468 - val_acc: 0.6314\n",
      "Epoch 90/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.0697 - acc: 0.6194\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00089: val_loss improved from 1.04675 to 1.04543, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00089: val_loss improved from 1.04675 to 1.04543, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00089: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00089: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00089: saving model to .\\training_callbacks/KERAS_check_model_epoch89.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0696 - acc: 0.6197 - val_loss: 1.0454 - val_acc: 0.6335\n",
      "Epoch 91/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0692 - acc: 0.6211\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00090: val_loss improved from 1.04543 to 1.04434, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00090: val_loss improved from 1.04543 to 1.04434, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00090: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00090: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0691 - acc: 0.6212 - val_loss: 1.0443 - val_acc: 0.6341\n",
      "Epoch 92/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0666 - acc: 0.6225\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00091: val_loss improved from 1.04434 to 1.04253, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00091: val_loss improved from 1.04434 to 1.04253, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00091: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00091: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0661 - acc: 0.6223 - val_loss: 1.0425 - val_acc: 0.6361\n",
      "Epoch 93/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0645 - acc: 0.6266\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00092: val_loss improved from 1.04253 to 1.04128, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00092: val_loss improved from 1.04253 to 1.04128, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00092: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00092: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0647 - acc: 0.6261 - val_loss: 1.0413 - val_acc: 0.6352\n",
      "Epoch 94/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0643 - acc: 0.6245\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00093: val_loss improved from 1.04128 to 1.03884, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00093: val_loss improved from 1.04128 to 1.03884, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00093: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00093: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0634 - acc: 0.6253 - val_loss: 1.0388 - val_acc: 0.6380\n",
      "Epoch 95/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.0625 - acc: 0.6259\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00094: val_loss improved from 1.03884 to 1.03715, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00094: val_loss improved from 1.03884 to 1.03715, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00094: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00094: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0621 - acc: 0.6259 - val_loss: 1.0371 - val_acc: 0.6388\n",
      "Epoch 96/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0615 - acc: 0.6262\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00095: val_loss improved from 1.03715 to 1.03670, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00095: val_loss improved from 1.03715 to 1.03670, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00095: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00095: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0617 - acc: 0.6265 - val_loss: 1.0367 - val_acc: 0.6388\n",
      "Epoch 97/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0597 - acc: 0.6286\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00096: val_loss improved from 1.03670 to 1.03451, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00096: val_loss improved from 1.03670 to 1.03451, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00096: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00096: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0598 - acc: 0.6286 - val_loss: 1.0345 - val_acc: 0.6404\n",
      "Epoch 98/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0576 - acc: 0.6293\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00097: val_loss improved from 1.03451 to 1.03242, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00097: val_loss improved from 1.03451 to 1.03242, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00097: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00097: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0571 - acc: 0.6290 - val_loss: 1.0324 - val_acc: 0.6422\n",
      "Epoch 99/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0553 - acc: 0.6316\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00098: val_loss improved from 1.03242 to 1.03074, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00098: val_loss improved from 1.03242 to 1.03074, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00098: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00098: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0552 - acc: 0.6317 - val_loss: 1.0307 - val_acc: 0.6427\n",
      "Epoch 100/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0556 - acc: 0.6308\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00099: val_loss improved from 1.03074 to 1.02947, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00099: val_loss improved from 1.03074 to 1.02947, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00099: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00099: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00099: saving model to .\\training_callbacks/KERAS_check_model_epoch99.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0552 - acc: 0.6303 - val_loss: 1.0295 - val_acc: 0.6439\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(options.inputModel)\n",
    "model.summary()\n",
    "startlearningrate=0.0001\n",
    "adam = Adam(lr=startlearningrate)\n",
    "model.compile(optimizer=adam, loss=[yamlConfig['KerasLoss']], metrics=['accuracy'])\n",
    "\n",
    "callbacks=all_callbacks(stop_patience=1000, \n",
    "                        lr_factor=0.5,\n",
    "                        lr_patience=10,\n",
    "                        lr_epsilon=0.000001, \n",
    "                        lr_cooldown=2, \n",
    "                        lr_minimum=0.0000001,\n",
    "                        outputDir=os.curdir + '\\\\training_callbacks')\n",
    "\n",
    "history = model.fit(X_train_val, y_train_val, batch_size = 1024, epochs = 100,\n",
    "                validation_split = 0.25, shuffle = True, callbacks = callbacks.callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeRoc(features_val, labels, labels_val, model):\n",
    "    print('in makeRoc()')\n",
    "    if 'j_index' in labels: labels.remove('j_index')\n",
    "\n",
    "    predict_test = model.predict(features_val)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    auc1 = {}\n",
    "    \n",
    "    plt.figure()       \n",
    "    for i, label in enumerate(labels):\n",
    "        df[label] = labels_val[:,i]\n",
    "        df[label + '_pred'] = predict_test[:,i]\n",
    "        \n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
    "\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "            \n",
    "        plt.plot(tpr[label],fpr[label],label='%s tagger, AUC = %.1f%%'%(label.replace('j_',''),auc1[label]*100.))\n",
    "    plt.semilogy()\n",
    "    plt.xlabel(\"Signal Efficiency\")\n",
    "    plt.ylabel(\"Background Efficiency\")\n",
    "    plt.ylim(0.001,1)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.figtext(0.25, 0.90,'hls4ml',fontweight='bold', wrap=True, horizontalalignment='right', fontsize=14)\n",
    "    #plt.figtext(0.35, 0.90,'preliminary', style='italic', wrap=True, horizontalalignment='center', fontsize=14) \n",
    "    plt.savefig(\"mygraph.png\")\n",
    "#     plt.savefig('%s/ROC.pdf'%(options.outputDir))\n",
    "#     plt.savefig('%s/ROC.pdf' %(outputDir))\n",
    "    return predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot ROC curve\n",
      "in makeRoc()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Plot ROC curve\")\n",
    "y_predict = makeRoc(X_test, labels, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(history):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(history.history['loss'], linewidth=1)\n",
    "    plt.plot(history.history['val_loss'], linewidth=1)\n",
    "    plt.title('Model Loss over Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['training sample loss','validation sample loss'])\n",
    "    plt.savefig('Learning_curve.pdf')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learningCurve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
